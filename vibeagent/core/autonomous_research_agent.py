"""Autonomous Research Agent for self-directed knowledge gathering.

This agent autonomously discovers, ingests, and organizes knowledge from
multiple sources to build a comprehensive knowledge base.
"""

import json
import logging
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any

from .agent import Agent
from .hybrid_search_orchestrator import HybridSearchOrchestrator
from .skill import SkillResult

logger = logging.getLogger(__name__)


@dataclass
class ResearchPlan:
    """Research plan generated by the agent."""

    topic: str
    subtopics: list[str] = field(default_factory=list)
    sources: list[str] = field(default_factory=list)
    search_queries: list[str] = field(default_factory=list)
    estimated_duration: int = 3600  # seconds
    priority: str = "medium"
    created_at: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class ResearchProgress:
    """Progress tracking for research tasks."""

    total_tasks: int
    completed_tasks: int
    sources_processed: int
    documents_ingested: int
    entities_extracted: int
    relationships_created: int
    errors: list[str] = field(default_factory=list)
    started_at: str = field(default_factory=lambda: datetime.now().isoformat())
    updated_at: str = field(default_factory=lambda: datetime.now().isoformat())


class AutonomousResearchAgent(Agent):
    """Agent for autonomous research and knowledge gathering."""

    def __init__(
        self,
        name: str = "AutonomousResearcher",
        llm_skill: Any | None = None,
        arxiv_skill: Any | None = None,
        firecrawl_skill: Any | None = None,
        qdrant_skill: Any | None = None,
        elasticsearch_skill: Any | None = None,
        neo4j_skill: Any | None = None,
        entity_extraction_skill: Any | None = None,
        hybrid_search: HybridSearchOrchestrator | None = None,
    ):
        """Initialize Autonomous Research Agent.

        Args:
            name: Agent name
            llm_skill: LLM skill for reasoning
            arxiv_skill: ArXiv search skill
            firecrawl_skill: Web scraping skill
            qdrant_skill: Vector storage skill
            elasticsearch_skill: Keyword search skill
            neo4j_skill: Knowledge graph skill
            entity_extraction_skill: Entity extraction skill
            hybrid_search: Hybrid search orchestrator
        """
        super().__init__(name=name)

        self.llm_skill = llm_skill
        self.arxiv_skill = arxiv_skill
        self.firecrawl_skill = firecrawl_skill
        self.qdrant_skill = qdrant_skill
        self.elasticsearch_skill = elasticsearch_skill
        self.neo4j_skill = neo4j_skill
        self.entity_extraction_skill = entity_extraction_skill
        self.hybrid_search = hybrid_search

        # Register skills
        if llm_skill:
            self.register_skill(llm_skill)
        if arxiv_skill:
            self.register_skill(arxiv_skill)
        if firecrawl_skill:
            self.register_skill(firecrawl_skill)
        if qdrant_skill:
            self.register_skill(qdrant_skill)
        if elasticsearch_skill:
            self.register_skill(elasticsearch_skill)
        if neo4j_skill:
            self.register_skill(neo4j_skill)
        if entity_extraction_skill:
            self.register_skill(entity_extraction_skill)

        self.current_progress = None

    def generate_research_plan(self, topic: str, depth: str = "medium") -> SkillResult:
        """Generate a research plan for a topic.

        Args:
            topic: Research topic
            depth: Research depth (shallow, medium, deep)

        Returns:
            SkillResult with research plan
        """
        try:
            if not self.llm_skill:
                return SkillResult(success=False, error="LLM skill not available")

            # Generate plan using LLM
            prompt = f"""Generate a comprehensive research plan for the topic: {topic}

Research depth: {depth}

Provide:
1. Main subtopics to investigate (5-10 items)
2. Types of sources to consult (arXiv papers, blog posts, documentation, etc.)
3. Specific search queries for each subtopic
4. Estimated duration in seconds

Respond with valid JSON:
{{
  "subtopics": ["Subtopic 1", "Subtopic 2", ...],
  "sources": ["source type 1", "source type 2", ...],
  "search_queries": ["query 1", "query 2", ...],
  "estimated_duration": 3600
}}"""

            result = self.llm_skill.execute(
                prompt=prompt,
                system_prompt="You are an expert research planner. Respond only with valid JSON.",
                max_tokens=2000,
            )

            if not result.success:
                return SkillResult(success=False, error=result.error)

            plan_data = json.loads(result.data.get("content", ""))

            plan = ResearchPlan(
                topic=topic,
                subtopics=plan_data.get("subtopics", []),
                sources=plan_data.get("sources", []),
                search_queries=plan_data.get("search_queries", []),
                estimated_duration=plan_data.get("estimated_duration", 3600),
            )

            return SkillResult(
                success=True,
                data={
                    "topic": plan.topic,
                    "subtopics": plan.subtopics,
                    "sources": plan.sources,
                    "search_queries": plan.search_queries,
                    "estimated_duration": plan.estimated_duration,
                },
            )

        except Exception as e:
            logger.error(f"Error generating research plan: {e}")
            return SkillResult(success=False, error=f"Plan generation failed: {str(e)}")

    def autonomous_research(self, topic: str, max_iterations: int = 10, **kwargs) -> SkillResult:
        """Perform autonomous research on a topic.

        Args:
            topic: Research topic
            max_iterations: Maximum research iterations

        Returns:
            SkillResult with research results
        """
        try:
            # Generate research plan
            plan_result = self.generate_research_plan(topic)
            if not plan_result.success:
                return plan_result

            plan = plan_result.data
            logger.info(f"Research plan generated for {topic}")

            # Initialize progress tracking
            self.current_progress = ResearchProgress(
                total_tasks=len(plan["search_queries"]),
                completed_tasks=0,
                sources_processed=0,
                documents_ingested=0,
                entities_extracted=0,
                relationships_created=0,
            )

            results = []

            # Execute research iterations
            for iteration, query in enumerate(plan["search_queries"][:max_iterations]):
                logger.info(f"Research iteration {iteration + 1}: {query}")

                iteration_result = self._research_iteration(query, **kwargs)
                results.append(iteration_result)

                # Update progress
                self.current_progress.completed_tasks += 1
                self.current_progress.updated_at = datetime.now().isoformat()

                if iteration_result.success:
                    self.current_progress.sources_processed += 1
                    self.current_progress.documents_ingested += iteration_result.data.get(
                        "documents_ingested", 0
                    )
                    self.current_progress.entities_extracted += iteration_result.data.get(
                        "entities_extracted", 0
                    )
                else:
                    self.current_progress.errors.append(iteration_result.error)

            # Generate summary
            summary_result = self._generate_research_summary(topic, results)

            self._record_usage()
            return SkillResult(
                success=True,
                data={
                    "topic": topic,
                    "plan": plan,
                    "results": results,
                    "summary": summary_result.data if summary_result.success else {},
                    "progress": {
                        "total_tasks": self.current_progress.total_tasks,
                        "completed_tasks": self.current_progress.completed_tasks,
                        "sources_processed": self.current_progress.sources_processed,
                        "documents_ingested": self.current_progress.documents_ingested,
                        "entities_extracted": self.current_progress.entities_extracted,
                        "relationships_created": self.current_progress.relationships_created,
                        "errors": self.current_progress.errors,
                    },
                },
            )

        except Exception as e:
            logger.error(f"Error in autonomous research: {e}")
            return SkillResult(success=False, error=f"Research failed: {str(e)}")

    def _research_iteration(self, query: str, **kwargs) -> SkillResult:
        """Execute a single research iteration.

        Args:
            query: Search query

        Returns:
            SkillResult with iteration results
        """
        documents_ingested = 0
        entities_extracted = 0
        sources = []

        try:
            # Search arXiv for papers
            if self.arxiv_skill:
                arxiv_result = self.arxiv_skill.execute(search=query, max_results=5)
                if arxiv_result.success:
                    papers = arxiv_result.data.get("papers", [])
                    for paper in papers:
                        source_id = self._ingest_document(paper, "arxiv")
                        if source_id:
                            sources.append(source_id)
                            documents_ingested += 1

            # Search web using Firecrawl
            if self.firecrawl_skill:
                # First, search for relevant URLs (would need a search API)
                # For now, scrape directly if a URL is provided
                web_urls = kwargs.get("web_urls", [])
                for url in web_urls[:3]:
                    scrape_result = self.firecrawl_skill.execute(action="scrape", url=url)
                    if scrape_result.success:
                        doc_id = self._ingest_document(scrape_result.data, "web")
                        if doc_id:
                            sources.append(doc_id)
                            documents_ingested += 1

            # Extract entities from ingested documents
            if sources and self.entity_extraction_skill:
                for source_id in sources:
                    # Get document content
                    # This would need to be implemented based on your storage
                    pass

            self._record_usage()
            return SkillResult(
                success=True,
                data={
                    "query": query,
                    "sources_found": len(sources),
                    "documents_ingested": documents_ingested,
                    "entities_extracted": entities_extracted,
                    "source_ids": sources,
                },
            )

        except Exception as e:
            logger.error(f"Error in research iteration: {e}")
            return SkillResult(success=False, error=f"Iteration failed: {str(e)}")

    def _ingest_document(self, document: dict, source_type: str) -> str | None:
        """Ingest a document into vector and knowledge graph stores.

        Args:
            document: Document data
            source_type: Type of source (arxiv, web, etc.)

        Returns:
            Document ID if successful, None otherwise
        """
        try:
            content = document.get("content", "")
            if not content:
                return None

            doc_id = f"{source_type}_{hash(content)}"

            # Store in vector database
            if self.qdrant_skill:
                self.qdrant_skill.execute(
                    action="insert",
                    documents=[{"id": doc_id, "content": content, "metadata": {"source": source_type, **document}}],
                )

            # Store in Elasticsearch for keyword search
            if self.elasticsearch_skill:
                self.elasticsearch_skill.execute(
                    action="insert",
                    documents=[
                        {
                            "id": doc_id,
                            "content": content,
                            "title": document.get("title", ""),
                            "url": document.get("url", ""),
                            "metadata": {"source": source_type, **document},
                        }
                    ],
                )

            # Extract entities and store in knowledge graph
            if self.neo4j_skill and self.entity_extraction_skill:
                extract_result = self.entity_extraction_skill.execute(action="extract", text=content)
                if extract_result.success:
                    entities = extract_result.data.get("entities", [])
                    relationships = extract_result.data.get("relationships", [])

                    # Insert entities
                    if entities:
                        self.neo4j_skill.execute(
                            action="insert_entity",
                            entities=[
                                {"id": e["id"], "label": e["label"], "properties": e}
                                for e in entities
                            ],
                        )

                    # Insert relationships
                    if relationships:
                        self.neo4j_skill.execute(
                            action="insert_relationship",
                            relationships=[
                                {
                                    "source_id": r["source_id"],
                                    "target_id": r["target_id"],
                                    "relationship_type": r["relationship_type"],
                                    "properties": r,
                                }
                                for r in relationships
                            ],
                        )

                    self.current_progress.entities_extracted += len(entities)
                    self.current_progress.relationships_created += len(relationships)

            return doc_id

        except Exception as e:
            logger.error(f"Error ingesting document: {e}")
            return None

    def _generate_research_summary(self, topic: str, results: list[SkillResult]) -> SkillResult:
        """Generate a summary of research results.

        Args:
            topic: Research topic
            results: Research iteration results

        Returns:
            SkillResult with summary
        """
        try:
            if not self.llm_skill:
                return SkillResult(success=False, error="LLM skill not available")

            # Prepare results for summarization
            results_summary = [
                {
                    "query": r.data.get("query"),
                    "sources": r.data.get("source_ids", []),
                    "documents": r.data.get("documents_ingested", 0),
                    "success": r.success,
                }
                for r in results
            ]

            prompt = f"""Generate a research summary for the topic: {topic}

Research Results:
{json.dumps(results_summary, indent=2)}

Provide:
1. Key findings (3-5 points)
2. Main themes discovered
3. Gaps or areas for further research
4. Overall assessment

Respond with valid JSON."""

            result = self.llm_skill.execute(
                prompt=prompt,
                system_prompt="You are an expert research summarizer. Respond only with valid JSON.",
                max_tokens=2000,
            )

            if not result.success:
                return SkillResult(success=False, error=result.error)

            summary_data = json.loads(result.data.get("content", ""))

            return SkillResult(
                success=True,
                data=summary_data,
            )

        except Exception as e:
            logger.error(f"Error generating summary: {e}")
            return SkillResult(success=False, error=f"Summary generation failed: {str(e)}")

    def query_knowledge_base(self, query: str, limit: int = 10) -> SkillResult:
        """Query the knowledge base using hybrid search.

        Args:
            query: Search query
            limit: Number of results

        Returns:
            SkillResult with query results
        """
        if self.hybrid_search:
            return self.hybrid_search.search(query, limit=limit)
        else:
            return SkillResult(success=False, error="Hybrid search not configured")

    def get_progress(self) -> dict:
        """Get current research progress.

        Returns:
            Progress information
        """
        if self.current_progress:
            return {
                "total_tasks": self.current_progress.total_tasks,
                "completed_tasks": self.current_progress.completed_tasks,
                "progress_percent": (
                    self.current_progress.completed_tasks / self.current_progress.total_tasks * 100
                    if self.current_progress.total_tasks > 0
                    else 0
                ),
                "sources_processed": self.current_progress.sources_processed,
                "documents_ingested": self.current_progress.documents_ingested,
                "entities_extracted": self.current_progress.entities_extracted,
                "relationships_created": self.current_progress.relationships_created,
                "errors": self.current_progress.errors,
                "started_at": self.current_progress.started_at,
                "updated_at": self.current_progress.updated_at,
            }
        return {}